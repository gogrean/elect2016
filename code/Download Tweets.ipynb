{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Clinton's and Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we download the tweets and tweet info of each candidate. The data is saved in a CSV file that can later be handled with `Pandas`. Not all the information in the pandas file will be useful. However, we err on the side of caution and save more information than we (immediately, at least) need, since missing information is difficult to add later on mostly due to the Twitter API.\n",
    "\n",
    "The API limits the user to accessing only the last 3200 tweets. To accumulate more data, the code needs to be run every few months to get the newest tweets. When that is done, it's important to check if a tweet that is already in the CSV file should be updated (e.g., because it received more favorites since we previously downloaded it).\n",
    "\n",
    "When a new tweet is added to the CSV file, it is appended at the end of the file. This means that the tweets saved will end up not being in chronological order. We can sort things later on if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a description of the fields in the CSV file:\n",
    "\n",
    "Column | Description\n",
    "--- | ---\n",
    "id | tweet id\n",
    "created_at | day, date, and time (UTC) when the tweet was posted\n",
    "source | device from which the tweet was posted\n",
    "text | text of tweet \n",
    "lang | language; for simplicity, only tweets in English are selected \n",
    "favorite_count | number of favorites the tweet received\n",
    "retweet_count | number of retweets the tweet received\n",
    "original_author | username under which the tweet was posted\n",
    "possibly_sensitive | boolean, always False (not True)\n",
    "hashtags | hashtags in the tweet\n",
    "user_mentions | Twitter users mentioned in the tweet\n",
    "place | place where the tweet was made based on the coordinates\n",
    "place_coord_boundaries | corner coordinates of the bounding box defining the location where the tweet was made "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Twitter API to download data requires a Twitter account and a mobile phone. The mobile phone number should be added to your Twitter account. If you have these and are willing to use them for the tutorial, then the next step is to create an application and get the keys needed to authorize requests to the Twitter API. \n",
    "\n",
    "Go to https://apps.twitter.com/ and click on `Create New App`.\n",
    "\n",
    "<img src=\"../img/create_new_app.jpg\">\n",
    "\n",
    "Enter all the information required in the fields marked with \\*, agree to the Twitter Development Agreement, and then click `Create your Twitter application`.\n",
    "\n",
    "<img src=\"../img/enter_app_info.jpg\">\n",
    "\n",
    "A new window will open to allow you to edit your application settings. Click on the `Keys and Access Tokens` tab at the top of the page.\n",
    "\n",
    "<img src=\"../img/keys_and_access_tokens.jpg\">\n",
    "\n",
    "At the bottom of the page, click on `Create my access token`. \n",
    "\n",
    "<img src=\"../img/create_access_token.jpg\">\n",
    "\n",
    "Now you should have all the tokens needed to use the API. You need the Consumer Key (API Key), Consumer Secret (API Secret), Access Token, and Access Token Secret. My advice is to save these in a YAML file rather than hard-code them. The YAML file will look something like this:\n",
    "\n",
    "<img src=\"../img/yaml_example.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load the credentials from the YAML file into Python, and authenticate in the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change this to match your own credentials filename.\n",
    "root_dir = os.path.dirname(os.getcwd()) + \"/\"\n",
    "twitter_cred = yaml.load(open(root_dir + 'credentials/twitter.cred'))\n",
    "\n",
    "auth = tweepy.OAuthHandler(twitter_cred['consumer_token'], \n",
    "                           twitter_cred['consumer_secret'])\n",
    "auth.set_access_token(twitter_cred['access_token_key'], \n",
    "                      twitter_cred['access_token_secret'])\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save the data in two separate CSV files: one for Trump's tweets and one for Clinton's tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trumps_tweets_file = \"data/trumps_tweets.csv\"\n",
    "clintons_tweets_file = \"data/clintons_tweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Column names of the CSV file.\n",
    "COLS = ['id', 'created_at', 'source', 'text', 'lang', \n",
    "        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags', \n",
    "        'user_mentions', 'place', 'place_coord_boundaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tweets(username, file):\n",
    "    # If the file exists, then read the existing data from the CSV file.\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header=0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=COLS)\n",
    "    \n",
    "    # The Twitter API limits the number of statuses one can download to\n",
    "    # about 200 per 'page'. Since we can download at most 3200 tweets, we \n",
    "    # loop through 17 pages to get our data.\n",
    "    for page in tweepy.Cursor(api.user_timeline, screen_name=username, \n",
    "                              count=200, include_rts=False).pages(17):\n",
    "        # Print some of the variables below to understand what's happening.\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    "            \n",
    "            # If the tweet was not written in English (either because it's actually \n",
    "            # in a different language or because it only contains links, hashtags, \n",
    "            # and/or user mentions), move on to the next tweet.\n",
    "            if status['lang'] != 'en':\n",
    "                continue\n",
    "\n",
    "            # Let's say that we want to update a tweet's info in the CSV file if the\n",
    "            # number of favorites and retweets has changed since previous download.\n",
    "            if status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
    "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                continue\n",
    "            \n",
    "            # If the tweet is written in English and is not already in the CSV file, \n",
    "            # add some of its info (the attributes that don't need to be processed) \n",
    "            # to the new_entry list.\n",
    "            new_entry += [status['id'], status['created_at'], \n",
    "                          status['source'], status['text'], status['lang'], \n",
    "                          status['favorite_count'], status['retweet_count']]\n",
    "            \n",
    "            # In this case, the original_author column will either have only\n",
    "            # realDonaldTrump or HillaryClinton, depending on the file. However, \n",
    "            # you can try to modify this function to save all the data in a single \n",
    "            # file. In that case, it's important to save the username.\n",
    "            new_entry.append(status['user']['screen_name'])\n",
    "            \n",
    "            try:\n",
    "                is_sensitive = status['possibly_sensitive']\n",
    "            except KeyError:\n",
    "                is_sensitive = None\n",
    "            new_entry.append(is_sensitive)\n",
    "            \n",
    "            # Save the hashtags and user mentions as comma-separated strings, e.g.,\n",
    "            #   \"MAGA, CrookedHillary\"\n",
    "            # for a Trump tweet.\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags)\n",
    "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "            new_entry.append(mentions)\n",
    "\n",
    "            try:\n",
    "                location = status['place']['full_name']    \n",
    "            except TypeError:\n",
    "                location = ''\n",
    "            new_entry.append(location)\n",
    "\n",
    "            try:\n",
    "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
    "            except TypeError:\n",
    "                coordinates = None\n",
    "            new_entry.append(coordinates)\n",
    "\n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "    df.to_csv(file, columns=COLS, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the function above to download Clinton's and Trump's most recent tweets. Their usernames are realDonaldTrump and HillaryClinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_tweets('realDonaldTrump', root_dir + 'data/trumps_tweets.csv')\n",
    "write_tweets('HillaryClinton', root_dir + 'data/clintons_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
